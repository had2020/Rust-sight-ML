fn relu(x: f64) -> f64 {
    if x > 0.0 {
        x
    } else {
        0.0
    }
}

fn main() {
    let inputs: Vec<f64> = vec![1.0, 2.0, 3.0]; // Incoming signals, could come from other neurons
    let mut weights: Vec<f64> = vec![0.5, -0.6, 0.2]; // Signal importance: determines whether to strengthen or weaken the signal
    let mut bias: f64 = 0.1; // Adjusts weight: determines whether the neuron should fire
    let target: f64 = 1.5; // Example target output

    let learning_rate: f64 = 0.01; // Learning rate for weight adjustments

    for epoch in 0..1000 { // Train for 1000 epochs
        // Forward pass
        let weighted_sum: f64 = inputs
            .iter()
            .zip(weights.iter())
            .map(|(input, weight)| input * weight)
            .sum::<f64>()
            + bias;

        let output = relu(weighted_sum); // Result after activation (action potential)

        // Compute loss (mean squared error)
        let loss = (output - target).powi(2);

        // Backpropagation
        let error_gradient = 2.0 * (output - target);
        let relu_gradient = if weighted_sum > 0.0 { 1.0 } else { 0.0 };
        let total_gradient = error_gradient * relu_gradient;

        // Update weights and bias
        for i in 0..weights.len() {
            weights[i] -= learning_rate * total_gradient * inputs[i];
        }
        bias -= learning_rate * total_gradient;

        // Display progress every 100 epochs
        if epoch % 100 == 0 {
            println!("Epoch {}: Loss = {}", epoch, loss);
        }
    }

    println!("Trained Weights: {:?}", weights);
    println!("Trained Bias: {}", bias);
}
